---
title: "From LLMs to AI: Making Sense of the Buzzwords"
date: 2025-08-08
tags: [sufficiently explained, ai]
---

I have recently found myself in rooms where the term “AI” means different things to different people, even within the same conversation. The rise of tools like ChatGPT, Claude, and Gemini has brought AI into the mainstream, but it has also blurred the definitions of terms like LLMs, GenAI, and Machine Learning. 

If you have ever felt confused by these terms, know that it is not your fault. The field of AI has always suffered from moving goalposts. On top of that, new breakthroughs reshape the language faster than most people can keep up with. But meaningful conversations rely on shared understanding, and when we all use the same words differently, we risk talking past each other.

{{< newsletter_signup >}}

That is why I am writing this post, as part of the first issue of my newsletter, *Sufficiently Explained*. The aim is to clarify what each of these terms mean and how they relate, so you can use them confidently in your next meeting!

This post takes a reverse approach: we will start with LLMs, the term people are most familiar with, and work our way backwards to the broader umbrella of AI.

## Large Language Models (LLMs)

These are powerful AI systems that can understand and generate human language. They have made AI accessible to everyone, with easy-to-use chat interfaces embedding them into our daily lives. Tools like ChatGPT, Claude, and Gemini all fall into this category.

LLMs are particularly good at answering questions, summarising documents, drafting emails and reports, translating between languages, and more. Trained on massive amounts of data, these models are designed to be general-purpose and adaptable across tasks.

That said, they aren’t perfect. LLMs can hallucinate facts, struggle with memory in long conversations, and carry a significant carbon footprint. Nevertheless, their capabilities continue to improving steadily.

## Generative AI (GenAI)

While LLMs focus on text, generative AI systems possess a broader set of creative abilities. We have built models that can create images (like DALL·E and MidJourney), videos (like Sora and Veo), and audio (like VoiceBox and AudioCraft).

These models are being used in genuinely creative ways, such as writing code, building dynamic game environments, composing songs, and more. For example, *Top Gun: Maverick* partnered with the London-based startup Sonantic (acquired by Spotify) to [recreate Val Kilmer’s voice using AI](https://fortune.com/2022/05/27/how-does-val-kilmer-speak-in-top-gun-maverick-sonantic-artificial-intelligence/) ([voice sample](https://youtu.be/OSMue60Gg6s?si=QHgOf9HVOSbTIutZ)).

This creative power also brings new risks and concerns. In 2023, a song that [went viral on TikTok appeared to be a collaboration between Drake and The Weeknd](https://www.bbc.co.uk/news/entertainment-arts-65298834), but it was actually generated using AI by an anonymous user. Creating content with AI is now easier and cheaper than ever. While AI-generated songs and artwork raise copyright concerns, at the darker end of the spectrum is the [rise in AI-generated image abuse and non-consensual content](https://www.abc.net.au/news/2024-06-15/ai-cyberbullying-revenge-porn-deepfake-nude-images-in-schools/103972244). As these models improve, it is becoming increasingly difficult to distinguish AI-generated media from the real thing.

## Deep Learning

All of the advanced AI systems mentioned above are powered by deep learning. At the heart of this field are artificial neural networks, which were originally inspired by the structure of biological brains. These networks are made by connecting many simple units, often called “neurons,” arranged in layers.

Deep learning models learn by adjusting connections between neurons based on the data they process, much like tuning many tiny knobs to get better at a task. The power of deep learning comes from stacking many layers of neurons, allowing the model recognise patterns of increasing complexity, from simple shapes to whole sentences or detailed images. Modern neural networks can have hundreds of layers.

Today, neural networks come in many types and flavours. Convolutional neural networks are great at understanding images, transformers excel at processing language, and diffusion models are used for creating realistic images and videos. 

Deep learning has become the foundation for many of today’s AI breakthroughs, enabling machines to see, hear, understand, and create content.

## Machine Learning (ML)

Machine Learning is a subset of AI focused on building algorithms that learn from data to make decisions or predictions without being explicitly programmed for every specific task. Instead of hardcoding rules, ML models find patterns in data and use those patterns to improve their performance over time.

Deep learning is a powerful part of this toolkit, but it is just one of many approaches. Traditional ML methods like linear regression, decision trees, random forests, support vector machines, and k-means clustering are still widely used across the industry in systems for fraud detection, credit scoring, recommendation systems, and more.

Traditional machine learning techniques are especially useful when working with structured data, like rows and columns in a spreadsheet. In contrast, deep learning tends to excel with unstructured data, such as images, audio, or natural language.

## Artificial Intelligence (AI)

[Coined in 1956](https://home.dartmouth.edu/about/artificial-intelligence-ai-coined-dartmouth), the term Artificial Intelligence refers to the field of building systems that mimic human intelligence. This includes the ability to perceive, reason, learn, plan, and act.

While today's conversations often focus on tools like LLMs and generative AI, the field of AI is much broader. It includes the data-driven methods of machine learning, but also techniques such as rule-based expert systems (like the [early chatbot *ELIZA*](https://liacademy.co.uk/the-story-of-eliza-the-ai-that-fooled-the-world/?v=7885444af42e)), evolutionary algorithms that simulate natural selection, and symbolic reasoning systems that manipulate logic-based rules. These earlier approaches are sometimes grouped under the label “Good Old-Fashioned AI” (GOFAI).

Many of these methods predate the deep learning revolution and powered early landmark systems like [*Deep Blue*](https://www.britannica.com/topic/Deep-Blue) (which defeated chess grandmaster Garry Kasparov in 1997) and [*Shakey the Robot*](https://www.sri.com/hoi/shakey-the-robot/) (a mobile robot from the 1960s that could perceive its surroundings, plan actions, and navigate its environment).

<img src="/images/posts/20250808_from_llms_to_ai/concentric_terms.jpg" class="large" alt="Concentric circles illustrating the relationship between AI concepts: the largest circle is Artificial Intelligence, containing Machine Learning, which in turn contains Deep Learning. Inside Deep Learning is Generative AI, and at the core are Large Language Models (LLMs).">
<em>Fig: How the AI concepts fit together: LLM < GenAI < Deep Learning < Machine Learning < AI</em>

Knowing how all these concepts connect is key to cutting through the AI buzz and ensuring we speak the same language when discussing AI. The diagram above represents these terms as concentric circles, showing how they fit together and making the entire AI ecosystem easier to grasp.

